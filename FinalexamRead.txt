실습을 통해 학습된 모델을 불러와 모델을 초기화 하고 미리 학습된 가중치를 읽어오고, 
FGSM 공격 코드를 입력해 원래 입력을 교란시켜 적대적인 예를 만드는 함수를 정의 할 수 있다.
그 다음 테스트 함수를 통해 엡실론 크기에 따라 공격자의 공격을 받는 모델의 정확도를 보고 받는다. 각각의 샘플에서 테스트 함수는 입력
데이터에 대한 손실 변화도를 계산하고 ,FGSM공격을 받은 작은 변화가 적용된 이미지를 만들고 나서 작은 변화가 적용된 이미지가 적대적인지 확인한다.
추가로 모델의 정확도를 테스트하기 위해서 테스트 함수는 나중에 시각화하여 볼 수 있도록 성공적으로 얻은 적대적 이미지를 저장하고 반환한다.
구현의 마지막 부분은 공격을 실행하는 것으로, 전체 테스트 스텝을 각 엡실론 값에 실행한다. 각 엡실론마다 최종 정확도와 성공적인 일부  적대 사례를
저장하여 다음 섹션에 표시하고, 엡실론 값이 증가함에 따라 출력된 정확도가 어떻게 감소하는지 볼 수 있다.
그로 인해 결과를 통해 엡실론이 증가함에 따라 테스트 정확도가 감소할 것이고, 엡실론이 클수록 손실을 극대화 하게 되는걸 확인할 수 있다.
따라서 엡실론이 증가할수록 테스트 정확도는 떨어지고, 작은 변화는 더 쉽게 인식할 수 있다는걸 알 수 있다.

이 결과에도 모든 경우에서 노이즈가 추가되더라도 사람은 올바르게 분류를 수행할 수 있고, 앞으로 적대적 공격과 방어 경쟁은 심화될것이기에
방어에 대한 연구는 자연스럽게 교란 및 해킹 목적으로 제작된 입력에 대해 머신 러닝 모델을 보다 견고하게 만들어야 할 것이다.
